FROM nvidia/cuda:12.6.3-cudnn-devel-ubuntu24.04

ENV TORCH_CUDA_ARCH_LIST="5.0"
ENV CUDA_ARCHITECTURES="50"
ENV LD_LIBRARY_PATH="/usr/lib:/usr/lib64:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

# Layer 0: System packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 python3.12-dev python3.12-venv python3-pip \
    git cmake build-essential \
    portaudio19-dev libportaudio2 \
    ffmpeg libsm6 libxext6 && \
    rm -rf /var/lib/apt/lists/*

# Layer 1: PyTorch 2.6.0 + UV
RUN --mount=type=cache,target=/pip-cache,id=pip-external \
    python3.12 -m venv /venv && \
    /venv/bin/pip install --cache-dir=/pip-cache uv && \
    /venv/bin/pip install --cache-dir=/pip-cache \
      torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \
      --index-url https://download.pytorch.org/whl/cu126 --no-deps

# Layer 2: Build llama-cpp-python from source with CUDA
RUN export CUDA_STUBS_DIR="/usr/local/cuda/lib64/stubs" && \
    ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    export LD_LIBRARY_PATH="$CUDA_STUBS_DIR:$LD_LIBRARY_PATH" && \
    /venv/bin/pip install --no-cache-dir scikit-build-core setuptools && \
    git clone https://github.com/abetlen/llama-cpp-python /tmp/llama-cpp-python && \
    cd /tmp/llama-cpp-python && \
    git submodule update --init --recursive && \
    sed -i '27s/"native"/"50"/' vendor/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt && \
    CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=50 -DGGML_NATIVE=OFF -DGGML_AVX2=ON -DCMAKE_EXE_LINKER_FLAGS=-L/usr/local/cuda/lib64/stubs" \
    /venv/bin/pip install --no-cache-dir --no-build-isolation --force-reinstall . && \
    cd / && rm -rf /tmp/llama-cpp-python

# Layer 3: Verify build artifacts and library linkage
RUN echo "=== Checking installed files ===" && \
    ls -lah /venv/lib/python3.12/site-packages/llama_cpp/ && \
    echo "" && \
    echo "=== Checking CUDA libraries ===" && \
    ls -lah /venv/lib/python3.12/site-packages/llama_cpp/lib/ && \
    echo "" && \
    echo "=== Verifying CUDA linkage for libggml-cuda.so ===" && \
    ldd /venv/lib/python3.12/site-packages/llama_cpp/lib/libggml-cuda.so | grep -E "cuda|not found" && \
    echo "" && \
    echo "=== Checking llama-cpp-python version ===" && \
    /venv/bin/pip show llama-cpp-python && \
    echo "" && \
    echo "=== Build artifacts verified ===" || \
    (echo "!!! VERIFICATION FAILED - See output above !!!" && exit 1)

# Layer 4: Flash-attn (optional, will fail on compute 5.0)
RUN /venv/bin/pip install --no-build-isolation flash-attn==2.6.3 2>&1 | tee /build_warnings.txt || \
    echo "âš  flash-attn install failed (expected on compute 5.0)" | tee -a /build_warnings.txt

ENV PATH="/venv/bin:${PATH}"
ENV VIRTUAL_ENV="/venv"

CMD ["bash"]