FROM nvidia/cuda:12.6.3-cudnn-devel-ubuntu24.04

ENV TORCH_CUDA_ARCH_LIST="5.0"
ENV CUDA_ARCHITECTURES="50"
ENV LD_LIBRARY_PATH="/usr/lib:/usr/lib64:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

# Layer 0: System packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 python3.12-dev python3.12-venv python3-pip \
    git cmake build-essential \
    portaudio19-dev libportaudio2 \
    ffmpeg libsm6 libxext6 \
    libtbb12 && \
    rm -rf /var/lib/apt/lists/*

# Layer 1: PyTorch 2.6.0 + UV
RUN --mount=type=cache,target=/pip-cache,id=pip-external \
    python3.12 -m venv /venv && \
    /venv/bin/pip install --cache-dir=/pip-cache uv && \
    /venv/bin/pip install --cache-dir=/pip-cache \
      torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \
      --index-url https://download.pytorch.org/whl/cu126 --no-deps

# Layer 2: Build llama-cpp-python from source with CUDA (DON'T delete yet)
RUN --mount=type=cache,target=/pip-cache,id=pip-external \
    export CUDA_STUBS_DIR="/usr/local/cuda/lib64/stubs" && \
    ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    export LD_LIBRARY_PATH="$CUDA_STUBS_DIR:$LD_LIBRARY_PATH" && \
    /venv/bin/pip install --cache-dir=/pip-cache scikit-build-core setuptools && \
    git clone https://github.com/JamePeng/llama-cpp-python /tmp/llama-cpp-python && \
    cd /tmp/llama-cpp-python && \
    git submodule update --init --recursive && \
    sed -i '27s/"native"/"50"/' vendor/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt && \
    CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=50 -DGGML_NATIVE=OFF -DGGML_AVX2=ON -DCMAKE_EXE_LINKER_FLAGS=-L/usr/local/cuda/lib64/stubs" \
    /venv/bin/pip install --cache-dir=/pip-cache --no-build-isolation --force-reinstall .

# Layer 2b: Install conversion requirements
RUN git clone https://github.com/ggerganov/llama.cpp /tmp/llama.cpp-main && \
    /venv/bin/pip install -r /tmp/llama.cpp-main/requirements/requirements-convert_hf_to_gguf.txt && \
    rm -rf /tmp/llama.cpp-main && \
    cd /tmp/llama-cpp-python/vendor/llama.cpp && \
    find requirements -name "*.txt" -exec /venv/bin/pip install -r {} \; 2>/dev/null || true

# Layer 2c: Build llama-quantize from vendor llama.cpp
RUN cd /tmp/llama-cpp-python/vendor/llama.cpp && \
    export CUDA_STUBS_DIR="/usr/local/cuda/lib64/stubs" && \
    export LD_LIBRARY_PATH="$CUDA_STUBS_DIR:$LD_LIBRARY_PATH" && \
    cmake -B build \
      -DGGML_CUDA=ON \
      -DCMAKE_CUDA_ARCHITECTURES=50 \
      -DGGML_NATIVE=OFF \
      -DLLAMA_CURL=OFF \
      -DCMAKE_EXE_LINKER_FLAGS="-L/usr/local/cuda/lib64/stubs" && \
    cmake --build build --config Release --target llama-quantize -j$(nproc) && \
    mkdir -p bin && cp build/bin/llama-quantize bin/ && \
    mv /tmp/llama-cpp-python/vendor/llama.cpp /opt/llama.cpp && \
    cd / && rm -rf /tmp/llama-cpp-python

# Layer 3: Verify build artifacts and library linkage
RUN echo "=== Checking installed files ===" && \
    ls -lah /venv/lib/python3.12/site-packages/llama_cpp/ && \
    echo "" && \
    echo "=== Checking CUDA libraries ===" && \
    ls -lah /venv/lib/python3.12/site-packages/llama_cpp/lib/ && \
    echo "" && \
    echo "=== Verifying CUDA linkage for libggml-cuda.so ===" && \
    ldd /venv/lib/python3.12/site-packages/llama_cpp/lib/libggml-cuda.so | grep -E "cuda|not found" && \
    echo "" && \
    echo "=== Checking llama-cpp-python version ===" && \
    /venv/bin/pip show llama-cpp-python && \
    echo "" && \
    echo "=== Build artifacts verified ===" || \
    (echo "!!! VERIFICATION FAILED - See output above !!!" && exit 1)

# Layer 4: Flash-attn (optional, will fail on compute 5.0)
RUN /venv/bin/pip install --no-build-isolation flash-attn==2.6.3 2>&1 | tee /build_warnings.txt || \
    echo "âš  flash-attn install failed (expected on compute 5.0)" | tee -a /build_warnings.txt

# Layer 5: Fix diffusers compatibility (solves the original import error)
RUN /venv/bin/pip install --upgrade diffusers

# Add llama library path - THIS IS THE FIX YOU WANTED
ENV LD_LIBRARY_PATH="/venv/lib/python3.12/site-packages/llama_cpp/lib:/usr/lib:/usr/lib64:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

ENV PATH="/venv/bin:${PATH}"
ENV VIRTUAL_ENV="/venv"

CMD ["bash"]