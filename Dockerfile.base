FROM nvidia/cuda:12.6.3-cudnn-devel-ubuntu24.04

# Set CUDA architecture for Quadro P5200 (compute 5.0)
ENV TORCH_CUDA_ARCH_LIST="5.0"

# Layer 0: System packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 python3.12-dev python3.12-venv python3-pip \
    git cmake build-essential \
    portaudio19-dev libportaudio2 \
    ffmpeg libsm6 libxext6 && \
    rm -rf /var/lib/apt/lists/*

# Layer 1: PyTorch 2.6.0 + flash-attn + UV
RUN --mount=type=cache,target=/pip-cache,id=pip-external \
    python3.12 -m venv /venv && \
    /venv/bin/pip install --cache-dir=/pip-cache uv && \
    /venv/bin/pip install --cache-dir=/pip-cache \
      torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \
      --index-url https://download.pytorch.org/whl/cu126 --no-deps && \
    /venv/bin/pip install --cache-dir=/pip-cache --no-build-isolation flash-attn==2.6.3 || \
    echo "âš  flash-attn install failed (expected on compute 5.0)" | tee /build_warnings.txt

# Layer 2: llama-cpp-python
COPY llama_cpp_python-0.3.18-cp312-cp312-linux_x86_64.whl /tmp/
RUN --mount=type=cache,target=/pip-cache,id=pip-external \
    /venv/bin/pip install --cache-dir=/pip-cache \
    /tmp/llama_cpp_python-0.3.18-cp312-cp312-linux_x86_64.whl && \
    rm /tmp/llama_cpp_python-0.3.18-cp312-cp312-linux_x86_64.whl

ENV PATH="/venv/bin:${PATH}"
ENV VIRTUAL_ENV="/venv"
ENV TORCH_CUDA_ARCH_LIST="5.0"

CMD ["bash"]