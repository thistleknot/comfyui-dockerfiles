FROM nvidia/cuda:12.6.3-cudnn-devel-ubuntu24.04

# Set CUDA architecture for Quadro P5200 (compute 5.0)
ENV TORCH_CUDA_ARCH_LIST="5.0"
ENV CUDA_ARCHITECTURES="50"
ENV LD_LIBRARY_PATH="/usr/lib:/usr/lib64:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

# Layer 0: System packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 python3.12-dev python3.12-venv python3-pip \
    git cmake build-essential \
    portaudio19-dev libportaudio2 \
    ffmpeg libsm6 libxext6 && \
    rm -rf /var/lib/apt/lists/*

# Layer 1: PyTorch 2.6.0 + UV
RUN --mount=type=cache,target=/pip-cache,id=pip-external \
    python3.12 -m venv /venv && \
    /venv/bin/pip install --cache-dir=/pip-cache uv && \
    /venv/bin/pip install --cache-dir=/pip-cache \
      torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \
      --index-url https://download.pytorch.org/whl/cu126 --no-deps

# Layer 2: Build llama.cpp CLI from source (compute 5.0)
RUN cd /tmp && \
    git clone https://github.com/ggml-org/llama.cpp && \
    cd llama.cpp && \
    cmake -B build \
      -DGGML_CUDA=ON \
      -DCMAKE_CUDA_ARCHITECTURES=50 \
      -DGGML_AVX2=ON \
      -DLLAMA_CURL=OFF && \
    cmake --build build --config Release -j$(nproc) && \
    cp build/bin/* /usr/local/bin/ 2>/dev/null || true && \
    cp build/lib/*.so /usr/local/lib/ 2>/dev/null || true && \
    ldconfig && \
    cd / && rm -rf /tmp/llama.cpp

# Layer 3: Build llama-cpp-python from source (compute 5.0) with CUDA stubs
RUN --mount=type=cache,target=/pip-cache,id=pip-external \
    export CUDA_STUBS_DIR="/usr/local/cuda/lib64/stubs" && \
    export LD_LIBRARY_PATH="$CUDA_STUBS_DIR:$LD_LIBRARY_PATH" && \
    export LDFLAGS="-L$CUDA_STUBS_DIR -lcuda" && \
    /venv/bin/pip install --cache-dir=/pip-cache scikit-build-core && \
    cd /tmp && \
    git clone https://github.com/abetlen/llama-cpp-python && \
    cd llama-cpp-python && \
    git submodule update --init --recursive && \
    CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=50 -DGGML_AVX2=ON -DCMAKE_EXE_LINKER_FLAGS=\"-L$CUDA_STUBS_DIR -lcuda\" -DCMAKE_SHARED_LINKER_FLAGS=\"-L$CUDA_STUBS_DIR -lcuda\"" \
    /venv/bin/pip install --cache-dir=/pip-cache --no-build-isolation . && \
    cd / && rm -rf /tmp/llama-cpp-python

# Layer 4: Verify llama-cpp-python installed (package check only)
RUN /venv/bin/pip show llama-cpp-python || \
    (echo "✗ llama-cpp-python not installed" && exit 1) && \
    echo "✓ llama-cpp-python installed successfully"

# Layer 5: Flash-attn (optional, will fail on compute 5.0)
RUN /venv/bin/pip install --no-build-isolation flash-attn==2.6.3 2>&1 | tee /build_warnings.txt || \
    echo "⚠ flash-attn install failed (expected on compute 5.0)" | tee -a /build_warnings.txt

ENV PATH="/venv/bin:${PATH}"
ENV VIRTUAL_ENV="/venv"

CMD ["bash"]